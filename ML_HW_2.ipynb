{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "import pandas as pd\n",
    "from numpy import log2 as log\n",
    "import pprint\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class $Dtrees$ is used to Create the dataset for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dtrees:\n",
    "    def __init__(self,k,m):\n",
    "        self.k = k    #number of features\n",
    "        self.m = m    #number of data points\n",
    "        self.arrofarrX = []\n",
    "        self.arrY = []\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            new = []\n",
    "            tmpX = self.generateX()\n",
    "            tmpW = self.generateW()\n",
    "            tmpY = self.computeY(tmpX,tmpW)\n",
    "            self.arrofarrX.append(tmpX)\n",
    "            self.arrY.append(tmpY)\n",
    "    \n",
    "    def ran(self,x,y):\n",
    "        if rd.uniform(x,y) > 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def ran2(self,x,y):\n",
    "        if rd.uniform(x,y) > 0.25:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def generateX(self):\n",
    "        new = []\n",
    "        if self.ran(0,1) == 1:\n",
    "            new.append(1)\n",
    "        else:\n",
    "            new.append(0)\n",
    "        \n",
    "        for x in range (1,self.k):\n",
    "            if self.ran2(0,1) == 1:\n",
    "                new.append(new[x-1])\n",
    "            else:\n",
    "                new.append(1-new[x-1])\n",
    "        return new\n",
    "    \n",
    "    def generateW(self):\n",
    "        new_w = []\n",
    "        for x in range (1,self.k):\n",
    "            de = 0\n",
    "            for y in range(1,x+1):\n",
    "                de = de+ math.pow(0.9,y)\n",
    "            res = math.pow(0.9,x) / de\n",
    "            new_w.append(res)\n",
    "        return new_w\n",
    "    \n",
    "    def computeY(self,arrX,arrW):\n",
    "        res = 0\n",
    "        for x in range (1,self.k):\n",
    "            res+= arrX[x]*arrW[x-1]\n",
    "#         print(\"current result is %d\",res)\n",
    "        if res >= 0.5:\n",
    "            return arrX[0]\n",
    "        else:\n",
    "            return 1 - arrX[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function used to Transfer data into data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDS(ds,arr):\n",
    "    tmp = len(arr)\n",
    "    for x in range(tmp):\n",
    "        ds[x] = arr[x]\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START BUILDING DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute $H(Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dtrees(20,5000)\n",
    "df = pd.DataFrame(d.arrofarrX)\n",
    "L = len(d.arrofarrX[0])\n",
    "df['Y'] = d.arrY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entropy(df):\n",
    "    ClFI = df.keys()[-1]   # to get the attribute name of classifcation column\n",
    "    entropy = 0\n",
    "    values = df[ClFI].unique()\n",
    "    for value in values:\n",
    "        fraction = df[ClFI].value_counts()[value]/len(df[ClFI])\n",
    "        entropy += -fraction*np.log2(fraction)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute $H(Y|X) = \\sum_{x}P(X = x)[-\\sum_{y}P(Y=y|X=x)\\log P(Y=y|X=x)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entropy_attribute(df,attribute):\n",
    "    ClFI = df.keys()[-1]      \n",
    "    target_variables = df[ClFI].unique()  \n",
    "    variables = df[attribute].unique()    \n",
    "    entropy2 = 0\n",
    "    for variable in variables:\n",
    "        entropy = 0\n",
    "        for target_variable in target_variables:\n",
    "            num = len(df[attribute][df[attribute]==variable][df[ClFI] ==target_variable])\n",
    "            den = len(df[attribute][df[attribute]==variable])\n",
    "            fraction = num/(den+eps)\n",
    "            entropy += -fraction*log(fraction+eps)\n",
    "        fraction2 = den/len(df)\n",
    "        entropy2 += -fraction2*entropy\n",
    "    return abs(entropy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the the attribute that could lead to the biggest entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_winner(df):\n",
    "    Entropy_att = []\n",
    "    IG = []\n",
    "    for key in df.keys()[:-1]:\n",
    "#         Entropy_att.append(find_entropy_attribute(df,key))\n",
    "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
    "    return df.keys()[:-1][np.argmax(IG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ans = find_winner(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get subtable since we find a attribute to cut the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtable(df, node,value):\n",
    "    return df[df[node] == value].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (DFS IMPLEMENTATION)FINAL STEP: LETS BUILD DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTree(df,tree=None): \n",
    "    Class = df.keys()[-1]   \n",
    "    \n",
    "    node = find_winner(df)   #get the attribute that could lead to he biggest entropy\n",
    "    \n",
    "    attValue = np.unique(df[node])   #get the distint attribute in the colomen which lead to the\n",
    "                                     #biggest entrophy\n",
    "    \n",
    "    #init a dictonary structure to store the entire decision tree\n",
    "    # which is a dictionary of dictionary\n",
    "    # the innder dictionary used to store the current layer of decision tree\n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {}\n",
    "    \n",
    "    #find the index of last column\n",
    "    \n",
    "    for value in attValue:\n",
    "        \n",
    "        subtable = get_subtable(df,node,value)\n",
    "        clValue,counts = np.unique(subtable['Y'],return_counts=True)    #based on the subtable, if counts of unique array\n",
    "                                                                        #equals one, means only one case exist to decide Y\n",
    "        if len(counts)==1:\n",
    "            tree[node][value] = clValue[0]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = buildTree(subtable) #DFS \n",
    "                   \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_arr,tree):     #predict function is just to traverse the TREE using DFS \n",
    "    #DFS\n",
    "    for node in tree.keys():\n",
    "        value = pred_arr[node]\n",
    "        tree = tree[node][value]\n",
    "        ans = 0\n",
    "        if type(tree) is dict:  # yet to reach the node\n",
    "            ans = predict(pred_arr,tree)\n",
    "        else:\n",
    "            ans = tree\n",
    "            break;\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_train(df,tree):\n",
    "    count = 0\n",
    "    l = len(df)\n",
    "    for x in range(l):\n",
    "        inst = df.iloc[x]\n",
    "        ans = predict(inst,tree)\n",
    "        if ans == inst[-1]:\n",
    "            count+=1\n",
    "        inst = 0\n",
    "    print((l-count)/l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0428\n"
     ]
    }
   ],
   "source": [
    "err_train(df,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_train2(df,tree):\n",
    "    count = 0\n",
    "    l = len(df)\n",
    "    for x in range(l):\n",
    "        inst = df.iloc[x]\n",
    "        ans = predict(inst,tree)\n",
    "        if ans == inst[-1]:\n",
    "            count+=1\n",
    "        inst = 0\n",
    "    return (l-count)/l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: The class $DTrees$ is used to generate the required dataset based on the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dtrees(20,10)                #Dtree(20,10) means number of features is 20, and number of data points is 30\n",
    "df = pd.DataFrame(d.arrofarrX)   #convert the generated dataset of x to dataframe\n",
    "L = len(d.arrofarrX[0])\n",
    "df['Y'] = d.arrY                 #concatenate Y to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9 ...  11  12  13  14  15  16  17  18  19  Y\n",
       "0  1  0  0  1  1  1  0  0  0  0 ...   0   0   1   0   0   0   0   1   1  1\n",
       "1  1  1  1  0  0  1  1  1  0  1 ...   1   0   0   0   1   1   0   0   0  1\n",
       "2  0  0  1  1  1  1  0  0  0  0 ...   0   0   0   0   0   0   0   0   0  0\n",
       "3  0  0  0  1  1  1  1  1  1  1 ...   0   0   0   0   0   1   1   1   0  0\n",
       "4  0  1  1  0  0  0  0  0  0  0 ...   1   1   1   0   0   0   0   0   0  0\n",
       "5  1  1  0  0  0  0  0  0  0  0 ...   0   0   1   1   1   0   0   0   0  1\n",
       "6  1  0  1  1  1  1  1  1  1  1 ...   0   0   0   0   0   0   0   1   1  1\n",
       "7  0  0  0  1  1  1  0  0  0  1 ...   1   1   1   0   0   1   0   0   1  0\n",
       "8  0  0  0  1  1  1  0  0  0  0 ...   1   1   1   1   1   1   0   0   0  0\n",
       "9  0  1  1  0  0  0  0  0  1  1 ...   1   1   1   0   0   0   0   0   1  0\n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df   #df is the dataframe that contains both $\\underline{X}_{m}$ and classification vection $\\underline{Y}_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: The function to train the Decision Tree is named $buildTree$, it has several helper functions :  {find_{entropy} : used to compute $H(Y)$}, {find_entropy_attribute: used to compute $H(Y|X)$}, {find_winner: used to find the attribute that could lead to the biggest information gain}, and {  get_subtable: used to get subtable based on the attribute input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildTree(df)    # call funcion buildTree to train decisiontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 0, 1: 1}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tree)    #this is the final decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In question (2) Below I compute the $err_{train}\\hat(f)$ which is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "err_train(df,tree)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question(3) First we generate dataset based on number of features is 4, and numer of data points is 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1  2  3  Y\n",
       "0   1  0  0  0  0\n",
       "1   0  0  0  1  1\n",
       "2   0  0  0  0  1\n",
       "3   0  0  0  1  1\n",
       "4   0  0  0  0  1\n",
       "5   1  1  1  1  1\n",
       "6   1  1  1  0  1\n",
       "7   0  0  1  0  1\n",
       "8   0  0  0  0  1\n",
       "9   0  0  0  0  1\n",
       "10  1  1  0  0  1\n",
       "11  1  1  1  1  1\n",
       "12  0  0  0  1  1\n",
       "13  1  1  1  1  1\n",
       "14  0  1  1  1  0\n",
       "15  0  1  1  1  0\n",
       "16  1  1  1  1  1\n",
       "17  1  1  0  0  1\n",
       "18  0  1  1  1  0\n",
       "19  1  1  1  1  1\n",
       "20  0  1  0  1  0\n",
       "21  0  1  0  0  0\n",
       "22  0  0  0  0  1\n",
       "23  1  0  0  1  0\n",
       "24  0  0  1  1  0\n",
       "25  1  1  0  0  1\n",
       "26  0  0  0  0  1\n",
       "27  0  0  1  0  1\n",
       "28  1  1  1  1  1\n",
       "29  1  1  0  0  1"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Dtrees(4,30)                #Dtree(20,10) means number of features is 20, and number of data points is 30\n",
    "df = pd.DataFrame(d.arrofarrX)   #convert the generated dataset of x to dataframe\n",
    "L = len(d.arrofarrX[0])\n",
    "df['Y'] = d.arrY \n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in Question 3, next We train the decision tree based on the input dataset we get right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildTree(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "err_train(df,tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In question 3, the ordering of variables in decision tree does not make sense, Because, Look at the $ID3$ algorithm, it only select the variable with maximum information gain to split the data, so each time to compute $IG$ our algorithm will traverse all variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: {0: {2: {0: {0: {0: {1: {0: 1, 1: 0}}, 1: {1: {0: 0, 1: 1}}}}, 1: 1}},\n",
      "     1: {0: {0: {1: {0: {2: {0: 1, 1: 0}}, 1: 0}}, 1: {1: {0: 0, 1: 1}}}}}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{3: {0: {2: {0: {0: {0: {1: {0: 1, 1: 0}}, 1: {1: {0: 0, 1: 1}}}}, 1: 1}},\n",
    "     1: {0: {0: {1: {0: {2: {0: 1, 1: 0}}, 1: 0}}, 1: {1: {0: 0, 1: 1}}}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the result of drawing decision tree based on the tree structure printed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./decision_tree.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"./decision_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bascially, I will first generate a dataset to train the decision tree, and generate another dataset to get the training error of decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dtrees(4,30)                #Dtree(4,30) means number of features is 4, and number of data points is 30\n",
    "df = pd.DataFrame(d.arrofarrX)   #convert the generated dataset of x to dataframe\n",
    "L = len(d.arrofarrX[0])\n",
    "df['Y'] = d.arrY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "tree = buildTree(df)\n",
    "err_train(df,tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new dataset for testing decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dtrees(4,50)              \n",
    "df = pd.DataFrame(d.arrofarrX)   #convert the generated dataset of x to dataframe\n",
    "L = len(d.arrofarrX[0])\n",
    "df['Y'] = d.arrY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset Below will not been peak by Decision tree before testing and in the process of testing the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03156\n"
     ]
    }
   ],
   "source": [
    "err_train(df,tree)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the result, the average error rate of this tree over the newly generated data without any peak from decision tree is $0.03156$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_history = []\n",
    "count = 0\n",
    "x_axis = []\n",
    "for m in range(10,1000):\n",
    "    d = Dtrees(4,m)              \n",
    "    df = pd.DataFrame(d.arrofarrX)   #convert the generated dataset of x to dataframe\n",
    "    L = len(d.arrofarrX[0])\n",
    "    df['Y'] = d.arrY \n",
    "    trainset, predictset = np.split(df, [int(.8*len(df))])   #80% training set, 20% predicting set\n",
    "    predictset.index = range(len(predictset.index))          #reindex the index of predicting set from zero to len-1\n",
    "    tree = buildTree(trainset)                               #build decision tree using training set\n",
    "    errtrain = err_train2(trainset,tree)\n",
    "    errpred = err_train2(predictset,tree)\n",
    "    err_history.append(math.fabs(errtrain-errpred))\n",
    "    x_axis.append(count)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The marginal value in the graph means as the value of $m$ increases which means the number of dataset that used to train decision tree increase, the decision tree can predict with higher accuracy, also we could find that the slope of graph below $->0$ as the $m$ increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1c3fe58400>]"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_axis, err_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In This Question, I will use Gini impority as the criterion for splitting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $Gini$ for formula is as follows: $I_{G}(P) =  \\sum_{i=1}^{J}p_{i}\\sum_{i=1}^{J}(1-p_{i})$ \n",
    "$i \\in{1,2,...,J}, $ and let $p{i}$ be the fraction of items labeled with class $i$ in the set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Gini(df,attribute):\n",
    "    ClFI = df.keys()[-1]\n",
    "    target_variables = df[ClFI].unique()\n",
    "    variables = df[attribute].unique()   #arr of atttribute from label column\n",
    "    IG = 0\n",
    "    for variable in variables:\n",
    "        TMP_IG = 0\n",
    "        for target_variable in target_variables:\n",
    "            num = len(df[attribute][df[attribute]==variable][df[ClFI] ==target_variable])\n",
    "            den = len(df[attribute][df[attribute]==variable])\n",
    "            fraction = num/(den + eps)\n",
    "            TMP_IG = fraction * (1-fraction)\n",
    "        IG += fraction * (1-fraction)\n",
    "    return abs(IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini_winner(df):\n",
    "    IG = []\n",
    "    for key in df.keys()[:-1]:\n",
    "#         Entropy_att.append(find_entropy_attribute(df,key))\n",
    "        IG.append(find_Gini(df,key))\n",
    "    return df.keys()[:-1][np.argmax(IG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IG_Tree(df,tree=None): \n",
    "    Class = df.keys()[-1]   \n",
    "    \n",
    "    node = Gini_winner(df)   #get the attribute that could lead to he biggest entropy\n",
    "    \n",
    "    attValue = np.unique(df[node])   #get the distint attribute in the colomen which lead to the\n",
    "                                     #biggest entrophy\n",
    "    \n",
    "    #init a dictonary structure to store the entire decision tree\n",
    "    # which is a dictionary of dictionary\n",
    "    # the innder dictionary used to store the current layer of decision tree\n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {}\n",
    "    \n",
    "    #find the index of last column\n",
    "    \n",
    "    for value in attValue:\n",
    "        \n",
    "        subtable = get_subtable(df,node,value)\n",
    "        clValue,counts = np.unique(subtable['Y'],return_counts=True)    #based on the subtable, if counts of unique array\n",
    "                                                                        #equals one, means only one case exist to decide Y\n",
    "        if len(counts)==1:\n",
    "            tree[node][value] = clValue[0]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = buildTree(subtable) #DFS \n",
    "                   \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_history = []\n",
    "count = 0\n",
    "x_axis = []\n",
    "for m in range(10,1000):\n",
    "    d = Dtrees(4,m)              \n",
    "    df = pd.DataFrame(d.arrofarrX)   #convert the generated dataset of x to dataframe\n",
    "    L = len(d.arrofarrX[0])\n",
    "    df['Y'] = d.arrY \n",
    "    trainset, predictset = np.split(df, [int(.8*len(df))])   #80% training set, 20% predicting set\n",
    "    predictset.index = range(len(predictset.index))          #reindex the index of predicting set from zero to len-1\n",
    "    tree = IG_Tree(trainset)                               #build decision tree using training set\n",
    "    errtrain = err_train2(trainset,tree)\n",
    "    errpred = err_train2(predictset,tree)\n",
    "    err_history.append(math.fabs(errtrain-errpred))\n",
    "    x_axis.append(count)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1c3fda1b70>]"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_axis, err_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compared the graph using Gini with using Information Gain, We could find that the slope of graph is similar, but in my experiment this time, the decision tree Using information Gain gets a smaller value of $|err_{train}(\\hat{f})-err(\\hat{f})|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
